---
title: "Addendum: Train illustrative predictive models"
author: "John LÃ¶vrot"
license: "CC BY 4.0"
date: "`r format(Sys.Date(), format = '%B %d, %Y')`"
output:
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
    fig_width: 10
    fig_height: 5
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.path = 'graphics/addendum-train-illustrative-models-',
  echo = TRUE, warning = FALSE, message = FALSE)
```

# Preparations

## Load data analysis project

Load project.

```{r load_project}
setwd("..")
ProjectTemplate::reload.project()
setwd("reports")
```

Load additional packages.

```{r}
library(caret)
library(glmnet)
library(randomForest)
```

http://topepo.github.io/caret/

## Settings

```{r}
trellis.par.set(caretTheme())
```

```{r}
trCntrl <- trainControl(
  method = "repeatedcv",
  number = 3,
  repeats = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary)
tune_length <- 5
n_keep <- 5000
```

## Prepare data

# Initial steps

## Non-specific filtering of features

```{r}
eset <- genefilter::featureFilter(casecontstudy)
eset <- genefilter::varFilter(eset, var.cutoff = 1 - n_keep / nrow(eset))
featureNames(eset) <- make.names(fData(eset)$probeid)
```

```{r}
print(eset)
```

```{r}
all_data <- cbind(
  pData(eset)[, "casecontcd", drop = FALSE],
  as.data.frame(t(exprs(eset))))
```

## Data splitting

The full study is split into a training set (2/3) and a test set (1/3), 
with the QC substudy part of the test set.

Step 1: 
Create initial partition of study subjects into training and test sets 
that are disjoint with respect to case-control sets, 
but not necessary at individual tumour level.

```{r}
all_casecontsets <- unique(casecontstudy$setnr)
casecontsets_not_in_qcsubstudy <- setdiff(all_casecontsets, 
  subset(pData(qcsubstudy), complete_casecontset)$setnr)

set.seed(20101216)
init_training_casecontsets <- sample(casecontsets_not_in_qcsubstudy, 
  size = length(all_casecontsets)*2/3)

init_training_subjids <- pData(casecontstudy) %>%
  filter(setnr %in% init_training_casecontsets) %>%
  dplyr::select(subjid) %>%
  unlist()
init_test_subjids <- setdiff(casecontstudy$subjid, init_training_subjids)
```

```{r}
tumorids_qcsubstudy <- filter(pData(qcsubstudy), 
  complete_casecontset)$tumorid
tumorids_cases_init_training <- filter(pData(casecontstudy), 
  casecontcd == "case" & subjid %in% init_training_subjids)$tumorid
tumorids_cases_init_test <- filter(pData(casecontstudy), 
  casecontcd == "case" & subjid %in% init_test_subjids)$tumorid
```

Step 2: 
Exclude some study subjects corresponding to same indivudal tumour such that 
the training and test sets also become disjoint at individual tumour level.

```{r}
## Identify tumours that correspond to study subjects in both training and test sets
tumorids_to_be_handled_tbl <- casecontstudy_design %>%
  mutate(
    init_partition = factor(
      subjid %in% init_training_subjids,
      levels = c(TRUE, FALSE),
      labels = c("init_training", "init_test"))
  ) %>%
  dplyr::select(tumorid, init_partition) %>%
  table() %>% 
  as.data.frame.matrix() %>%
  rownames_to_column(var = "tumorid") %>%
  as_tibble() %>%
  dplyr::rename(
    n_subj_init_training = init_training,
    n_subj_init_test = init_test
  ) %>%
  rowwise() %>%
  mutate(
    in_qcsubstudy = is.element(tumorid, tumorids_qcsubstudy),
    case_in_init_training = is.element(tumorid, tumorids_cases_init_training),
    case_in_init_test = is.element(tumorid, tumorids_cases_init_test)
  ) %>%
  filter(n_subj_init_training > 0 & n_subj_init_test > 0)
```

```{r}
## Decide
tumorids_to_be_handled_tbl <- tumorids_to_be_handled_tbl %>%
  mutate(
    excl_subj_from_training = !case_in_init_training &
      (in_qcsubstudy | case_in_init_test | (n_subj_init_training <= n_subj_init_test)),
    excl_subj_from_test = !excl_subj_from_training
  )
```

```{r, echo = FALSE}
if (interactive()) {
  nrow(tumorids_to_be_handled_tbl)
  tumorids_to_be_handled_tbl %>%
    summary()
  tumorids_to_be_handled_tbl %>%
    arrange(desc(n_subj_init_training + n_subj_init_test)) %>%
    head() %>%
    knitr::kable()
}
```

```{r}
## Assemble final training set
tumorids_to_be_exl_from_training <- tumorids_to_be_handled_tbl %>%
  filter(excl_subj_from_training) %>%
  dplyr::select(tumorid) %>%
  unlist()

subjids_to_be_excl_from_training <- casecontstudy_design %>%
  filter(tumorid %in% tumorids_to_be_exl_from_training) %>%
  dplyr::select(subjid) %>%
  unlist()

training_subjids <- 
  setdiff(init_training_subjids, subjids_to_be_excl_from_training)

training <- all_data[training_subjids, ]
```

```{r}
## Assemble final test set
tumorids_to_be_exl_from_test <- tumorids_to_be_handled_tbl %>%
  filter(excl_subj_from_test) %>%
  dplyr::select(tumorid) %>%
  unlist()

subjids_to_be_excl_from_test <- casecontstudy_design %>%
  filter(tumorid %in% tumorids_to_be_exl_from_test) %>%
  dplyr::select(subjid) %>%
  unlist()

test_subjids <- 
  setdiff(init_test_subjids, subjids_to_be_excl_from_test)

testing <- all_data[test_subjids, ]
```

```{r, echo = FALSE}
casecontsets <- unique(casecontstudy$setnr)
tibble(
  casecontset = casecontsets, 
  partition = factor(
    ifelse(casecontset %in% pData(casecontstudy)[rownames(training), "setnr"], 
      "Training set",
      ifelse(casecontset %in% pData(casecontstudy)[rownames(testing), "setnr"], 
        "Test set", NA)),
    levels = c("Training set", "Test set")), 
  in_qcsubstudy = factor(
    casecontset %in% subset(pData(qcsubstudy), complete_casecontset)$setnr, 
    levels = c(TRUE, FALSE), 
    labels = c("QC substudy", "(rest)"))) %>%
  dplyr::select(in_qcsubstudy, partition) %>%
  table(useNA = "ifany") %>%
  addmargins() %>%
  knitr::kable(caption = "Table. Number of case-control sets in each partition of the full study.")
```

Indidivudal tumours: 

```{r, echo = FALSE}
tumorids <- unique(casecontstudy$tumorid)
tibble(
  tumorid = tumorids, 
  tumour_in_training_set = tumorid %in% 
    pData(casecontstudy)[rownames(training), "tumorid"],
  tumour_in_test_set = tumorid %in% 
    pData(casecontstudy)[rownames(testing), "tumorid"]) %>%
  dplyr::select(tumour_in_training_set, tumour_in_test_set) %>%
  table(useNA = "ifany") %>%
  addmargins()
```

# Investigated models

Panel of models:

* Linear Discriminant Analysis (LDA) with Selection by Univarite Filter
    * as an example of a simple -- yet for gene-expression data often supprisingly powerful -- approach
* Logistic Regression with Elastic Net Regularisation
    * as an example of advanced method with implicit feature selection
* Random Forest with Selection by Univarite Filter
    * as an example of another advanced approach
* (LDA based on the) HER2 amplicon metagene
    * for comparison and as a positive control

Compare with, for example, 
http://topepo.github.io/caret/feature-selection-overview.html#feature-selection-methods

# Train models

See also
http://topepo.github.io/caret/model-training-and-tuning.html

Initialise container to store models.

```{r}
fit_list <- list()
```

Construct stratified cross-validation folds of study subjects that 
are disjoint with respect to case-control sets. 
However, the folds are not necessary disjoint at individual tumour level. 
Therefore, the performance as estimated by this cross-validation approach is 
most likely slightly over-optimistic.

```{r, echo = FALSE}
if(interactive()) {
  test_folds <- groupKFold(
    group = pData(casecontstudy)[rownames(training), "setnr"],
    k = trCntrl$number)
  
  pData(casecontstudy)[rownames(training), c("setnr", "subjid")] %>%
    mutate(Fold1 = subjid %in% rownames(training)[test_folds[["Fold1"]]]) %>%
    dplyr::select(setnr, Fold1) %>%
    table() %>%
    head(n = 20)
}
```

```{r}
createMultiGroupKFold <- function(group, k = 10, times = 5) {
  ## Adopted from caret::createMultiFolds
  pretty_nums <- paste0("Rep", gsub(" ", "0", format(1:times)))
  for (i in 1:times) {
    tmp <- groupKFold(group, k = k)
    names(tmp) <- paste0(
      "Fold", gsub(" ", "0", format(seq(along = tmp))), 
      ".", pretty_nums[i])
    out <- if (i == 1) 
      tmp
    else 
      c(out, tmp)
  }
  out
}

set.seed(20101216)
my_folds <- createMultiGroupKFold(
  group = pData(casecontstudy)[rownames(training), "setnr"],
  k = trCntrl$number, 
  times = trCntrl$repeats)

trCntrl$index <- my_folds
```

## Model: HER2 amplicon metagene

```{r HER2_amplicon_metagene}
fit_list[["HER2 amplicon metagene"]] <-
  train(casecontcd ~ HER2_amplicon_metagene,
    data = pData(eset)[rownames(training), c("casecontcd", "HER2_amplicon_metagene")],
    method = "lda",
    metric = "ROC",
    trControl = trCntrl)

print(fit_list[["HER2 amplicon metagene"]])
```

## Model: Linear Discriminant Analysis with Selection by Univarite Filter

```{r ldaSBF}
ldaSBF2 <- ldaSBF
ldaSBF2$score <- function(x, y)
  t.test(x ~ y)$p.value
ldaSBF2$filter <- function (score, x, y)
  p.adjust(score, "bonferroni") < 0.05  # stringent criterion
ldaSBF2$summary <- twoClassSummary

filterCtrl <- sbfControl(
  functions = ldaSBF2,
  method = trCntrl$method,
  number = trCntrl$number,
  repeats = trCntrl$repeats, 
  index = trCntrl$index)

fit_list[["Linear Discriminant Analysis with Selection by Univariate Filter"]] <-
  sbf(casecontcd ~ ., data = training, sbfControl = filterCtrl)

print(fit_list[["Linear Discriminant Analysis with Selection by Univariate Filter"]])
```

## Model: Logistic Regression with Elastic Net Regularisation

```{r glmnet}
fit_list[["Logistic Regression with Elastic Net Regularisation"]] <-
  train(casecontcd ~ ., data = training,
    method = "glmnet",
    family = "binomial",
    metric = "ROC",
    trControl = trCntrl,
    tuneLength = tune_length,
    NULL)

plot(fit_list[["Logistic Regression with Elastic Net Regularisation"]])
```

## Model: Random Forest with Selection by Univarite Filter

```{r rfSBF}
rfSBF2 <- rfSBF
rfSBF2$score <- function(x, y)
  t.test(x ~ y)$p.value
rfSBF2$filter <- function (score, x, y)
  p.adjust(score, "BH") < 0.25  # less stringent criterion
rfSBF2$summary <- twoClassSummary

fit_list[["Random Forest with Selection by Univariate Filter"]] <-
  sbf(casecontcd ~ ., data = training,
    sbfControl = sbfControl(
      functions = rfSBF2,
      method = trCntrl$method,
      number = trCntrl$number,
      repeats = trCntrl$repeats, 
      index = trCntrl$index))

print(fit_list[["Random Forest with Selection by Univariate Filter"]])
```

# Compare models

Again, 
the performance as estimated by this cross-validation approach is most likely slightly over-optimistic. 
The reason is that the cross-validation folds not necessarily are disjoint at individual tumour level.

```{r}
resamps <- resamples(fit_list)
parallelplot(resamps, metric = "ROC")
dotplot(resamps, metric = "ROC")
```

See also
http://topepo.github.io/caret/model-training-and-tuning.html#between-models

# Cache results

```{r}
save(testing, training, fit_list, 
  file = file.path("..", "cache", "caret", "training.RData"))
```

# R session information

```{r}
print(sessionInfo(), locale = FALSE)
```

- - -

&copy; 2017 John LÃ¶vrot.  
This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).  
The source code is available at [github.com/lovrot/reproduce-cunha15canres](http://github.com/lovrot/reproduce-cunha15canres).  
Version `r format(read.dcf("../description.dcf")[1, "version"])`
